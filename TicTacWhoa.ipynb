{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TicTacWhoa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v65z8sWihJGG",
        "colab_type": "text"
      },
      "source": [
        "# Objective: The goal of this notebook is to teach an Agent to play tic-tac-toe. \n",
        "\n",
        "**Methodology**: We implement [Q-learning](https://en.wikipedia.org/wiki/Q-learning) from scratch with optional epsilon-greedy training, and ensemble modeling. The agent learns to play either first or second against a random player.\n",
        "\n",
        "**Observations**: When teaching the agent to play in a single session, the agent learns fairly variant behaviors for starting a game as the first player and would suggest that 1) there are either several equally optimal first moves or 2) learning against a random player teaches the agent different behaviors, reinforcing the winning behaviors randomly. This led to implementing an ensemble approach to see if the agent could consistently find the most optimal first move. Several ensembles settled on a first move at the center of the board, which is known to be an acceptable first move according to known [strategy](https://en.wikipedia.org/wiki/Tic-tac-toe#Strategy). However, playing the opening move in a corner gives the opponent more chances to make a mistake. Even so either move does not make a difference between perfect players. I, personally, lost more often when the agent opened with a corner move but found it hard, if not impossible, to win when playing the opening move at all.\n",
        "\n",
        "**Drawbacks**: The main drawback of this approach in general is that all states must be stored in memory. It's interesting that the number of unique states (when the agent must learn to play first or second) is 8,533. Given the seemingly small total action space at each step, it's easy to see how much one would need to save to memory to teach an agent a much more complex game.\n",
        "\n",
        "**Next**: \n",
        "* Compare these results to that of deep Q-learning.\n",
        "* Train two agents at the same time against each other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sG0BsCapg9az",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXFU6hnygsW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IskDxnnzg_1C",
        "colab_type": "text"
      },
      "source": [
        "# Define TicTacWhoa Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzwmMpo1gzeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TicTacWhoa:\n",
        "  def __init__(self, lr=.9, gamma=.9, epsilon=.2, episodes=100000):\n",
        "   \"\"\"Set learning rate, reward discount (gamma), probability of random move (epsilon),\n",
        "   and number of episodes for training.\n",
        "   \"\"\"\n",
        "   self.lr = lr\n",
        "   self.gamma = gamma\n",
        "   self.epsilon = epsilon\n",
        "   self.episodes = episodes\n",
        "\n",
        "  def _create_board(self):\n",
        "    \"\"\"Initialize clear board and check state space.\n",
        "    \"\"\"\n",
        "    self.done = False\n",
        "    self.board = np.array([3,4,5,6,7,8,9,10,11])\n",
        "    self._check_or_add_state()\n",
        "\n",
        "  def _check_or_add_state(self):\n",
        "    \"\"\"Check or add current board state in state dictionary. Set current action space.\n",
        "    \"\"\"\n",
        "    self.state = str(list(self.board))\n",
        "    if self.state_dict.get(self.state) is None:\n",
        "      self.state_dict[self.state] = np.zeros(len(self.board[self.board > 2]))\n",
        "    self.action_space = self.state_dict[self.state]\n",
        "    if len(self.action_space) == 0:\n",
        "      self.action_space = np.array([0])\n",
        "\n",
        "  def _play_random(self, player):\n",
        "    \"\"\"Make a random play and check state space.\n",
        "    \"\"\"\n",
        "    random_ind = random.choice(np.where(self.board > 2)[0])\n",
        "    self.board[random_ind] = player\n",
        "    self._check_or_add_state()\n",
        "  \n",
        "  def _choose_action_play_agent(self, possible_random=True):\n",
        "    \"\"\"Have agent choose an action.\n",
        "    \"\"\"\n",
        "    if (possible_random) and (np.random.choice([1,2,3,4,5,6,7,8,9,10]) <= (self.epsilon*10)):\n",
        "      self.action = np.argmax(np.random.rand(1,len(self.action_space)))\n",
        "    else:\n",
        "      if possible_random:\n",
        "        self.action = np.argmax(self.action_space + np.random.randn(1,len(self.action_space))*(1./(self.i+1)))\n",
        "      else:\n",
        "        self.action = np.argmax(self.action_space)\n",
        "    self.board[self.board[self.board > 2][self.action] - 3] = 1 \n",
        "    self._check_or_add_state()\n",
        "\n",
        "  def _is_finished(self):\n",
        "    \"\"\"Determine whether the game is over. Set reward.\n",
        "    \"\"\"\n",
        "    self.reward = 0\n",
        "    self.winner = 0\n",
        "\n",
        "    if len(self.board[self.board>2]) == 0:\n",
        "      self.winner = 3\n",
        "      self.done = True\n",
        "\n",
        "    shaped_board = self.board.reshape(3,3)\n",
        "    for i in range(3):\n",
        "      set_row = set(shaped_board[i])\n",
        "      if len(set_row) == 1:\n",
        "        self.winner = list(set_row)[0]\n",
        "      set_col =  set(shaped_board[:,i])\n",
        "      if len(set_col) == 1:\n",
        "        self.winner = list(set_col)[0]\n",
        "\n",
        "    set_left_diag = set(shaped_board.reshape(3,3).diagonal(0))\n",
        "    if len(set_left_diag) == 1:\n",
        "        self.winner = list(set_left_diag)[0]\n",
        "\n",
        "    set_right_diag = set(np.flipud(shaped_board.reshape(3,3)).diagonal(0))\n",
        "    if len(set_right_diag) == 1:\n",
        "        self.winner = list(set_right_diag)[0]\n",
        "\n",
        "    if self.winner == 2:\n",
        "      self.reward = -1\n",
        "      self.done = True\n",
        "    elif self.winner == 1:\n",
        "      self.reward = 1\n",
        "      self.done = True\n",
        "\n",
        "  def _step(self):\n",
        "    \"\"\"Move the game one step further. Determine if game is over.\n",
        "    \"\"\"\n",
        "    self._is_finished()\n",
        "    if self.winner == 0:\n",
        "      self._play_random(2)\n",
        "      self._is_finished()\n",
        "\n",
        "  def _update_state_dict(self, current_state):\n",
        "    \"\"\"Update state dictionary with current information.\n",
        "    \"\"\"\n",
        "    self.state_dict[current_state][self.action] = (1-self.lr)*self.state_dict[current_state][self.action] \\\n",
        "                                                  + self.lr*(self.reward \\\n",
        "                                                  + self.gamma*np.max(self.action_space))\n",
        "\n",
        "  def learn(self, ensemble=False):\n",
        "    \"\"\"Teach agent to play tic-tac-toe. Agent will learn to go first and second.\n",
        "    Train multiple agents with the ensemble parameter.\n",
        "    \"\"\"\n",
        "    self.state_dict = {}\n",
        "    self.rewards_lists = [np.zeros(self.episodes), np.zeros(self.episodes)]\n",
        "    for i in range(self.episodes):\n",
        "      self.i = i\n",
        "      for j in range(2):\n",
        "        self._create_board()\n",
        "        if j == 1:\n",
        "          self._step()\n",
        "        for _ in range(9):\n",
        "          current_state = self.state\n",
        "          self._choose_action_play_agent()\n",
        "          self._step()\n",
        "          self._update_state_dict(current_state)\n",
        "          if self.done == True:\n",
        "              break\n",
        "        self.rewards_lists[j][i] += self.reward\n",
        "    if ensemble:\n",
        "      return self.state_dict\n",
        "\n",
        "  def learn_ensemble(self, repeats):\n",
        "    \"\"\"Teach multiple agents to play tic-tac-toe and combine them into one agent.\n",
        "    \"\"\"\n",
        "    self.state_dict_ensemble = {}\n",
        "    for i in tqdm(range(repeats)):\n",
        "      state_dict = self.learn(ensemble=True)\n",
        "      for state, actions in state_dict.items():\n",
        "        if self.state_dict_ensemble.get(state) is not None:\n",
        "          self.state_dict_ensemble[state] += actions\n",
        "        else:\n",
        "           self.state_dict_ensemble[state] = actions\n",
        "    self.state_dict = self.state_dict_ensemble\n",
        "\n",
        "  def _display_board(self):\n",
        "    \"\"\"Display current board.\n",
        "    \"\"\"\n",
        "    new_board = []\n",
        "    for b in self.board:\n",
        "      if b == 1:\n",
        "        new_board.append('X')\n",
        "      elif b == 2:\n",
        "        new_board.append('O')\n",
        "      else:\n",
        "        new_board.append(' ')\n",
        "    print(\"\"\"\n",
        "     {} | {} | {} \n",
        "    ---|---|---\n",
        "     {} | {} | {} \n",
        "    ---|---|---\n",
        "     {} | {} | {} \n",
        "    \"\"\".format(new_board[0], new_board[1], new_board[2],\n",
        "               new_board[3], new_board[4], new_board[5],\n",
        "               new_board[6], new_board[7], new_board[8]))\n",
        "\n",
        "  def _play_computer(self):\n",
        "    \"\"\"Have the agent make a move.\n",
        "    \"\"\"\n",
        "    print('\\nComputer made a move:')\n",
        "    self._choose_action_play_agent(possible_random=False)\n",
        "\n",
        "  def _play_human(self):\n",
        "    \"\"\"Allow human to make a move.\n",
        "    \"\"\"\n",
        "    print('\\nYou made a move:')\n",
        "    ind = int(input('Select the index of your move: '))\n",
        "    self.board[ind] = 2\n",
        "    self._check_or_add_state()\n",
        "\n",
        "  def play(self):\n",
        "    \"\"\"Play a game against the agent.\n",
        "    \"\"\"\n",
        "    self._create_board()\n",
        "    self._display_board()\n",
        "    whos_first = input('Do you want to go first? (y/n): ')\n",
        "    for i in range(9):\n",
        "      if i % 2 == 0:\n",
        "        if whos_first == 'y':\n",
        "          self._play_human()\n",
        "        else:\n",
        "          self._play_computer()\n",
        "      else:\n",
        "        if whos_first == 'y':\n",
        "          self._play_computer()\n",
        "        else:\n",
        "          self._play_human()\n",
        "      self._display_board()\n",
        "      self._is_finished()\n",
        "      if self.done == True:\n",
        "        print(\"Game Over!\")\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPGR02zJhDi8",
        "colab_type": "text"
      },
      "source": [
        "# Teach Agent to play, and play against it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLnBqzIRg1E0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ttw = TicTacWhoa(episodes=10000, epsilon=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJbz6wM7g3Bb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0394a0cf-55c4-4503-d14d-0ef921abd889"
      },
      "source": [
        "ttw.learn_ensemble(repeats=100)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [13:23<00:00,  8.03s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcINwudboeoz",
        "colab_type": "text"
      },
      "source": [
        "We let the agent play first, and it chooses the middle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51Te3FTng4VZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f0cc52bd-3982-4629-dfdd-eaa00a268d5c"
      },
      "source": [
        "ttw.play()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "       |   |   \n",
            "    ---|---|---\n",
            "       |   |   \n",
            "    ---|---|---\n",
            "       |   |   \n",
            "    \n",
            "Do you want to go first? (y/n): n\n",
            "\n",
            "Computer made a move:\n",
            "\n",
            "       |   |   \n",
            "    ---|---|---\n",
            "       | X |   \n",
            "    ---|---|---\n",
            "       |   |   \n",
            "    \n",
            "\n",
            "You made a move:\n",
            "Select the index of your move: 0\n",
            "\n",
            "     O |   |   \n",
            "    ---|---|---\n",
            "       | X |   \n",
            "    ---|---|---\n",
            "       |   |   \n",
            "    \n",
            "\n",
            "Computer made a move:\n",
            "\n",
            "     O |   |   \n",
            "    ---|---|---\n",
            "       | X |   \n",
            "    ---|---|---\n",
            "     X |   |   \n",
            "    \n",
            "\n",
            "You made a move:\n",
            "Select the index of your move: 2\n",
            "\n",
            "     O |   | O \n",
            "    ---|---|---\n",
            "       | X |   \n",
            "    ---|---|---\n",
            "     X |   |   \n",
            "    \n",
            "\n",
            "Computer made a move:\n",
            "\n",
            "     O | X | O \n",
            "    ---|---|---\n",
            "       | X |   \n",
            "    ---|---|---\n",
            "     X |   |   \n",
            "    \n",
            "\n",
            "You made a move:\n",
            "Select the index of your move: 7\n",
            "\n",
            "     O | X | O \n",
            "    ---|---|---\n",
            "       | X |   \n",
            "    ---|---|---\n",
            "     X | O |   \n",
            "    \n",
            "\n",
            "Computer made a move:\n",
            "\n",
            "     O | X | O \n",
            "    ---|---|---\n",
            "       | X | X \n",
            "    ---|---|---\n",
            "     X | O |   \n",
            "    \n",
            "\n",
            "You made a move:\n",
            "Select the index of your move: 3\n",
            "\n",
            "     O | X | O \n",
            "    ---|---|---\n",
            "     O | X | X \n",
            "    ---|---|---\n",
            "     X | O |   \n",
            "    \n",
            "\n",
            "Computer made a move:\n",
            "\n",
            "     O | X | O \n",
            "    ---|---|---\n",
            "     O | X | X \n",
            "    ---|---|---\n",
            "     X | O | X \n",
            "    \n",
            "Game Over!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U6OrPuOo3dQ",
        "colab_type": "text"
      },
      "source": [
        "Or we can play first and choose the corner. [The optimal move for the agent at this point is the center as well](https://en.wikipedia.org/wiki/Tic-tac-toe#Strategy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-g5cjAuoAib",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "77a0f2aa-be99-4e0d-c5ec-45f4084fe796"
      },
      "source": [
        "ttw.play()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "       |   |   \n",
            "    ---|---|---\n",
            "       |   |   \n",
            "    ---|---|---\n",
            "       |   |   \n",
            "    \n",
            "Do you want to go first? (y/n): y\n",
            "\n",
            "You made a move:\n",
            "Select the index of your move: 0\n",
            "\n",
            "     O |   |   \n",
            "    ---|---|---\n",
            "       |   |   \n",
            "    ---|---|---\n",
            "       |   |   \n",
            "    \n",
            "\n",
            "Computer made a move:\n",
            "\n",
            "     O |   |   \n",
            "    ---|---|---\n",
            "       | X |   \n",
            "    ---|---|---\n",
            "       |   |   \n",
            "    \n",
            "\n",
            "You made a move:\n",
            "Select the index of your move: 8\n",
            "\n",
            "     O |   |   \n",
            "    ---|---|---\n",
            "       | X |   \n",
            "    ---|---|---\n",
            "       |   | O \n",
            "    \n",
            "\n",
            "Computer made a move:\n",
            "\n",
            "     O |   |   \n",
            "    ---|---|---\n",
            "       | X |   \n",
            "    ---|---|---\n",
            "       | X | O \n",
            "    \n",
            "\n",
            "You made a move:\n",
            "Select the index of your move: 1\n",
            "\n",
            "     O | O |   \n",
            "    ---|---|---\n",
            "       | X |   \n",
            "    ---|---|---\n",
            "       | X | O \n",
            "    \n",
            "\n",
            "Computer made a move:\n",
            "\n",
            "     O | O | X \n",
            "    ---|---|---\n",
            "       | X |   \n",
            "    ---|---|---\n",
            "       | X | O \n",
            "    \n",
            "\n",
            "You made a move:\n",
            "Select the index of your move: 6\n",
            "\n",
            "     O | O | X \n",
            "    ---|---|---\n",
            "       | X |   \n",
            "    ---|---|---\n",
            "     O | X | O \n",
            "    \n",
            "\n",
            "Computer made a move:\n",
            "\n",
            "     O | O | X \n",
            "    ---|---|---\n",
            "     X | X |   \n",
            "    ---|---|---\n",
            "     O | X | O \n",
            "    \n",
            "\n",
            "You made a move:\n",
            "Select the index of your move: 5\n",
            "\n",
            "     O | O | X \n",
            "    ---|---|---\n",
            "     X | X | O \n",
            "    ---|---|---\n",
            "     O | X | O \n",
            "    \n",
            "Game Over!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejxRJR7Ap1Ji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}